{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Goal:\n",
    "In this notebook, we want to revise and modify the rnn **regression** notebook we learned from mofan's video.  \n",
    "The **goal** is to grab a better understanding of RNN as a prediction model.  \n",
    "The **task** is then:  \n",
    "1. modify the format of input to simulate our financial data\n",
    "2. change the layer dimensions accordingly to fit the model (challenging part.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Toy model test\n",
    "Here, we generate some fake data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 12])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(1, 100, 12)\n",
    "print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0138, -0.4167,  1.4651,  0.4104, -0.4140, -1.0407, -0.4664,\n",
       "          -0.3975, -0.8543,  1.1309, -0.7778, -1.6786],\n",
       "         [ 0.9704,  0.1135,  0.6346,  0.3732, -0.4819,  0.8748,  0.1210,\n",
       "           1.1366, -0.3635,  1.0505,  0.8145, -0.2515]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0138, -0.4167,  1.4651,  ...,  1.1309, -0.7778, -1.6786],\n",
       "        [ 0.9704,  0.1135,  0.6346,  ...,  1.0505,  0.8145, -0.2515],\n",
       "        [-2.7264,  0.9686, -1.0000,  ...,  2.0921,  1.2037, -0.3476],\n",
       "        ...,\n",
       "        [-0.8164, -0.0762,  1.3119,  ..., -0.6137, -0.1876,  1.9818],\n",
       "        [-0.9342,  0.9342,  0.9005,  ...,  0.6509,  0.2531,  0.8515],\n",
       "        [-0.2573,  0.0044,  1.6089,  ..., -1.5131,  2.4126,  0.8745]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[0].size())\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "BATCH_SIZE = 1\n",
    "TIME_STEP = 10\n",
    "INPUT_SIZE = 12\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(12, 30, batch_first=True)\n",
      "  (out): Linear(in_features=30, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 30,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.out = nn.Linear(30, 12) # is the first number here = hidden_size? and second = 1 for regression?\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "#         outs = []\n",
    "#         for time_step in range(r_out.size(1)): #???...hum..why not range(time_step?)\n",
    "#             outs.append(self.out(r_out[:, time_step, :]))\n",
    "#         return torch.stack(outs, dim=1), h_state\n",
    "        r_out = r_out.view(-1,30)\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "    \n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "h_state = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Step:  1 | train loss: 1.4086\n",
      "2\n",
      "Step:  2 | train loss: 0.8989\n",
      "3\n",
      "Step:  3 | train loss: 1.3688\n",
      "4\n",
      "Step:  4 | train loss: 0.9468\n",
      "5\n",
      "Step:  5 | train loss: 0.9278\n",
      "6\n",
      "Step:  6 | train loss: 1.1741\n",
      "7\n",
      "Step:  7 | train loss: 1.5252\n",
      "8\n",
      "Step:  8 | train loss: 1.4569\n",
      "9\n",
      "Step:  9 | train loss: 0.9410\n",
      "10\n",
      "Step:  10 | train loss: 0.9517\n",
      "11\n",
      "Step:  11 | train loss: 1.1464\n",
      "12\n",
      "Step:  12 | train loss: 2.0098\n",
      "13\n",
      "Step:  13 | train loss: 2.0307\n",
      "14\n",
      "Step:  14 | train loss: 0.7553\n",
      "15\n",
      "Step:  15 | train loss: 1.2335\n",
      "16\n",
      "Step:  16 | train loss: 0.9237\n",
      "17\n",
      "Step:  17 | train loss: 0.9132\n",
      "18\n",
      "Step:  18 | train loss: 2.2163\n",
      "19\n",
      "Step:  19 | train loss: 0.7320\n",
      "20\n",
      "Step:  20 | train loss: 1.6227\n",
      "21\n",
      "Step:  21 | train loss: 1.4013\n",
      "22\n",
      "Step:  22 | train loss: 0.8143\n",
      "23\n",
      "Step:  23 | train loss: 1.0847\n",
      "24\n",
      "Step:  24 | train loss: 1.0021\n",
      "25\n",
      "Step:  25 | train loss: 0.7223\n",
      "26\n",
      "Step:  26 | train loss: 0.9300\n",
      "27\n",
      "Step:  27 | train loss: 0.5650\n",
      "28\n",
      "Step:  28 | train loss: 1.4096\n",
      "29\n",
      "Step:  29 | train loss: 0.5424\n",
      "30\n",
      "Step:  30 | train loss: 0.7389\n",
      "31\n",
      "Step:  31 | train loss: 0.6562\n",
      "32\n",
      "Step:  32 | train loss: 0.9375\n",
      "33\n",
      "Step:  33 | train loss: 0.8835\n",
      "34\n",
      "Step:  34 | train loss: 0.7432\n",
      "35\n",
      "Step:  35 | train loss: 1.0775\n",
      "36\n",
      "Step:  36 | train loss: 1.5435\n",
      "37\n",
      "Step:  37 | train loss: 0.9551\n",
      "38\n",
      "Step:  38 | train loss: 0.5914\n",
      "39\n",
      "Step:  39 | train loss: 1.0283\n",
      "40\n",
      "Step:  40 | train loss: 1.5437\n",
      "41\n",
      "Step:  41 | train loss: 0.4727\n",
      "42\n",
      "Step:  42 | train loss: 1.3844\n",
      "43\n",
      "Step:  43 | train loss: 1.1181\n",
      "44\n",
      "Step:  44 | train loss: 1.3731\n",
      "45\n",
      "Step:  45 | train loss: 1.3819\n",
      "46\n",
      "Step:  46 | train loss: 1.1728\n",
      "47\n",
      "Step:  47 | train loss: 1.1783\n",
      "48\n",
      "Step:  48 | train loss: 0.9940\n",
      "49\n",
      "Step:  49 | train loss: 1.1809\n",
      "50\n",
      "Step:  50 | train loss: 1.2731\n",
      "51\n",
      "Step:  51 | train loss: 1.4325\n",
      "52\n",
      "Step:  52 | train loss: 0.4278\n",
      "53\n",
      "Step:  53 | train loss: 0.8791\n",
      "54\n",
      "Step:  54 | train loss: 0.9990\n",
      "55\n",
      "Step:  55 | train loss: 0.5940\n",
      "56\n",
      "Step:  56 | train loss: 1.0195\n",
      "57\n",
      "Step:  57 | train loss: 1.4708\n",
      "58\n",
      "Step:  58 | train loss: 0.8168\n",
      "59\n",
      "Step:  59 | train loss: 0.5738\n",
      "60\n",
      "Step:  60 | train loss: 1.9559\n",
      "61\n",
      "Step:  61 | train loss: 0.7534\n",
      "62\n",
      "Step:  62 | train loss: 0.9255\n",
      "63\n",
      "Step:  63 | train loss: 2.5706\n",
      "64\n",
      "Step:  64 | train loss: 1.2007\n",
      "65\n",
      "Step:  65 | train loss: 1.1234\n",
      "66\n",
      "Step:  66 | train loss: 1.3974\n",
      "67\n",
      "Step:  67 | train loss: 0.7469\n",
      "68\n",
      "Step:  68 | train loss: 0.5082\n",
      "69\n",
      "Step:  69 | train loss: 1.6849\n",
      "70\n",
      "Step:  70 | train loss: 0.8095\n",
      "71\n",
      "Step:  71 | train loss: 0.8399\n",
      "72\n",
      "Step:  72 | train loss: 1.0720\n",
      "73\n",
      "Step:  73 | train loss: 1.0422\n",
      "74\n",
      "Step:  74 | train loss: 1.2977\n",
      "75\n",
      "Step:  75 | train loss: 1.3562\n",
      "76\n",
      "Step:  76 | train loss: 0.7037\n",
      "77\n",
      "Step:  77 | train loss: 0.9878\n",
      "78\n",
      "Step:  78 | train loss: 0.6037\n",
      "79\n",
      "Step:  79 | train loss: 1.3781\n",
      "80\n",
      "Step:  80 | train loss: 0.7574\n",
      "81\n",
      "Step:  81 | train loss: 1.2704\n",
      "82\n",
      "Step:  82 | train loss: 1.0315\n",
      "83\n",
      "Step:  83 | train loss: 1.1290\n",
      "84\n",
      "Step:  84 | train loss: 0.6333\n",
      "85\n",
      "Step:  85 | train loss: 0.5665\n",
      "86\n",
      "Step:  86 | train loss: 1.3170\n",
      "87\n",
      "Step:  87 | train loss: 0.4944\n",
      "88\n",
      "Step:  88 | train loss: 1.1843\n",
      "89\n",
      "Step:  89 | train loss: 1.1769\n",
      "90\n",
      "Step:  90 | train loss: 1.8618\n",
      "91\n",
      "Step:  91 | train loss: 0.7328\n",
      "92\n",
      "Step:  92 | train loss: 0.4967\n",
      "93\n",
      "Step:  93 | train loss: 1.1784\n",
      "94\n",
      "Step:  94 | train loss: 1.2036\n",
      "95\n",
      "Step:  95 | train loss: 1.1693\n",
      "96\n",
      "Step:  96 | train loss: 0.7857\n",
      "97\n",
      "Step:  97 | train loss: 0.9306\n",
      "98\n",
      "Step:  98 | train loss: 0.5735\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "for i in range(1, data.size(1)//BATCH_SIZE-1):\n",
    "# for i in range(1, 200):    \n",
    "    print(i)\n",
    "    x = data[:,((i-1)*BATCH_SIZE):(i * BATCH_SIZE),:]\n",
    "    y = data[:,(i * BATCH_SIZE+1),:]\n",
    "    \n",
    "    prediction, h_state = rnn(x, h_state)\n",
    "    h_state = h_state.data\n",
    "    \n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step() \n",
    "#     if step %1 == 0:\n",
    "    print('Step: ', i, '| train loss: %.4f' % loss.data.numpy())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1470,  0.3870, -0.1489,  0.2920,  0.2054, -0.1866, -0.1912,  0.4354,\n",
      "         -0.0209, -0.7571, -0.0532, -0.1190]], grad_fn=<ThAddmmBackward>)\n",
      "tensor([[ 0.2689, -0.4908, -0.2760,  0.7299, -0.9108,  0.4038, -0.5539, -0.4288,\n",
      "          1.2873,  0.2868,  0.3210,  0.5704]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1, figsize=(12, 5))\n",
    "# plt.ion()           # continuously plot\n",
    "\n",
    "# for step in range(100):\n",
    "#     start, end = step * np.pi, (step+1)*np.pi   # time range\n",
    "#     # use sin predicts cos\n",
    "#     steps = np.linspace(start, end, TIME_STEP, dtype=np.float32)\n",
    "#     x_np = np.sin(steps)    # float32 for converting torch FloatTensor\n",
    "#     y_np = np.cos(steps)\n",
    "\n",
    "#     x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n",
    "#     y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n",
    "\n",
    "#     prediction, h_state = rnn(x, h_state)   # rnn output\n",
    "#     # !! next step is important !!\n",
    "#     h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
    "\n",
    "#     loss = loss_func(prediction, y)         # calculate loss\n",
    "#     optimizer.zero_grad()                   # clear gradients for this training step\n",
    "#     loss.backward()                         # backpropagation, compute gradients\n",
    "#     optimizer.step()                        # apply gradients\n",
    "\n",
    "#     # plotting\n",
    "# #     plt.plot(steps, y_np.flatten(), 'r-')\n",
    "# #     plt.plot(steps, prediction.data.numpy().flatten(), 'b-')\n",
    "# #     plt.draw(); plt.pause(0.05)\n",
    "#     if step %20 == 0:\n",
    "#         print('Step: ', step, '| train loss: %.4f' % loss.data.numpy())\n",
    "# # plt.ioff()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1470,  0.3870, -0.1489,  0.2920,  0.2054, -0.1866, -0.1912,\n",
       "           0.4354, -0.0209, -0.7571, -0.0532, -0.1190]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Using SPY500 data from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4721, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>1469.250000</td>\n",
       "      <td>1478.000000</td>\n",
       "      <td>1438.359985</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>931800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1397.430054</td>\n",
       "      <td>1399.420044</td>\n",
       "      <td>1399.420044</td>\n",
       "      <td>1009000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>1399.420044</td>\n",
       "      <td>1413.270020</td>\n",
       "      <td>1377.680054</td>\n",
       "      <td>1402.109985</td>\n",
       "      <td>1402.109985</td>\n",
       "      <td>1085500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>1402.109985</td>\n",
       "      <td>1411.900024</td>\n",
       "      <td>1392.099976</td>\n",
       "      <td>1403.449951</td>\n",
       "      <td>1403.449951</td>\n",
       "      <td>1092300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>1403.449951</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>1400.729980</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>1225200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2000-01-03  1469.250000  1478.000000  1438.359985  1455.219971   \n",
       "1  2000-01-04  1455.219971  1455.219971  1397.430054  1399.420044   \n",
       "2  2000-01-05  1399.420044  1413.270020  1377.680054  1402.109985   \n",
       "3  2000-01-06  1402.109985  1411.900024  1392.099976  1403.449951   \n",
       "4  2000-01-07  1403.449951  1441.469971  1400.729980  1441.469971   \n",
       "\n",
       "     Adj Close      Volume  \n",
       "0  1455.219971   931800000  \n",
       "1  1399.420044  1009000000  \n",
       "2  1402.109985  1085500000  \n",
       "3  1403.449951  1092300000  \n",
       "4  1441.469971  1225200000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/toby/Documents/PycharmProjects/Pi3Orion/SP500_01012000_10072018.csv\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4721 entries, 0 to 4720\n",
      "Data columns (total 6 columns):\n",
      "Date      4721 non-null object\n",
      "Open      4721 non-null float64\n",
      "High      4721 non-null float64\n",
      "Low       4721 non-null float64\n",
      "Close     4721 non-null float64\n",
      "Volume    4721 non-null int64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 221.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(\"Adj Close\",1)\n",
    "data.Volume = data.Volume // 10000000\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1469.250000</td>\n",
       "      <td>1478.000000</td>\n",
       "      <td>1438.359985</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1397.430054</td>\n",
       "      <td>1399.420044</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1399.420044</td>\n",
       "      <td>1413.270020</td>\n",
       "      <td>1377.680054</td>\n",
       "      <td>1402.109985</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1402.109985</td>\n",
       "      <td>1411.900024</td>\n",
       "      <td>1392.099976</td>\n",
       "      <td>1403.449951</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1403.449951</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>1400.729980</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open         High          Low        Close  Volume\n",
       "0  1469.250000  1478.000000  1438.359985  1455.219971      93\n",
       "1  1455.219971  1455.219971  1397.430054  1399.420044     100\n",
       "2  1399.420044  1413.270020  1377.680054  1402.109985     108\n",
       "3  1402.109985  1411.900024  1392.099976  1403.449951     109\n",
       "4  1403.449951  1441.469971  1400.729980  1441.469971     122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 1, without any feature engineering\n",
    "spy_df = data.drop(\"Date\",1)\n",
    "spy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4721, 1, 5])\n",
      "torch.Size([1, 4721, 5])\n"
     ]
    }
   ],
   "source": [
    "spy_df = torch.tensor(spy_df.values).unsqueeze(dim = 1).float()\n",
    "print(spy_df.size())\n",
    "spy_df = spy_df.view([1,-1,5])\n",
    "print(spy_df.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4721, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1469.2500, 1478.0000, 1438.3600, 1455.2200,   93.0000],\n",
       "        [1455.2200, 1455.2200, 1397.4301, 1399.4200,  100.0000],\n",
       "        [1399.4200, 1413.2700, 1377.6801, 1402.1100,  108.0000],\n",
       "        ...,\n",
       "        [2931.6899, 2939.8601, 2921.3601, 2925.5100,  359.0000],\n",
       "        [2919.3501, 2919.7800, 2883.9199, 2901.6101,  349.0000],\n",
       "        [2902.5400, 2909.6399, 2869.2900, 2885.5701,  332.0000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spy_df[0].size())\n",
    "spy_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 30, num_layers=2, batch_first=True)\n",
      "  (rnn2): RNN(5, 30, num_layers=2, batch_first=True)\n",
      "  (out): Linear(in_features=30, out_features=5, bias=True)\n",
      ")\n",
      "updating new train loss:  1566121.2\n",
      "updating new train loss:  1559401.9\n",
      "updating new train loss:  1515224.2\n",
      "updating new train loss:  1506604.0\n",
      "updating new train loss:  1484876.1\n",
      "updating new train loss:  1440741.0\n",
      "updating new train loss:  1434633.2\n",
      "updating new train loss:  1428354.1\n",
      "Step:  100 | train loss: 1487941.75\n",
      "updating new train loss:  1391246.1\n",
      "updating new train loss:  1367705.0\n",
      "Step:  200 | train loss: 1367705.00\n",
      "updating new train loss:  1349735.1\n",
      "updating new train loss:  1344077.2\n",
      "updating new train loss:  1325633.1\n",
      "updating new train loss:  1313934.5\n",
      "updating new train loss:  1243140.1\n",
      "updating new train loss:  1213885.4\n",
      "updating new train loss:  1197235.4\n",
      "updating new train loss:  1161576.2\n",
      "updating new train loss:  1146781.0\n",
      "updating new train loss:  1131532.4\n",
      "updating new train loss:  1076785.5\n",
      "updating new train loss:  1041257.4\n",
      "updating new train loss:  1027780.56\n",
      "Step:  300 | train loss: 1027780.56\n",
      "updating new train loss:  1013966.2\n",
      "updating new train loss:  995796.6\n",
      "updating new train loss:  992117.56\n",
      "updating new train loss:  989740.5\n",
      "updating new train loss:  944396.75\n",
      "updating new train loss:  908239.4\n",
      "updating new train loss:  893655.9\n",
      "Step:  400 | train loss: 1050901.25\n",
      "updating new train loss:  892421.44\n",
      "updating new train loss:  852741.0\n",
      "updating new train loss:  838747.94\n",
      "updating new train loss:  808994.2\n",
      "updating new train loss:  759310.25\n",
      "updating new train loss:  733057.0\n",
      "updating new train loss:  705108.25\n",
      "updating new train loss:  662090.5\n",
      "Step:  500 | train loss: 945329.88\n",
      "Step:  600 | train loss: 775887.88\n",
      "updating new train loss:  660924.56\n",
      "updating new train loss:  649168.2\n",
      "updating new train loss:  646803.2\n",
      "updating new train loss:  623920.75\n",
      "updating new train loss:  605242.3\n",
      "updating new train loss:  590301.3\n",
      "updating new train loss:  576717.3\n",
      "updating new train loss:  553468.94\n",
      "updating new train loss:  542823.8\n",
      "updating new train loss:  537631.56\n",
      "updating new train loss:  517756.9\n",
      "updating new train loss:  484125.4\n",
      "updating new train loss:  445580.2\n",
      "updating new train loss:  419161.1\n",
      "updating new train loss:  417753.53\n",
      "updating new train loss:  408879.75\n",
      "updating new train loss:  390261.75\n",
      "updating new train loss:  390005.75\n",
      "updating new train loss:  383445.44\n",
      "Step:  700 | train loss: 489318.22\n",
      "updating new train loss:  383156.8\n",
      "Step:  800 | train loss: 421493.50\n",
      "Step:  900 | train loss: 574488.69\n",
      "Step:  1000 | train loss: 751554.12\n",
      "Step:  1100 | train loss: 713880.75\n",
      "Step:  1200 | train loss: 715699.50\n",
      "Step:  1300 | train loss: 857540.38\n",
      "Step:  1400 | train loss: 869236.38\n",
      "Step:  1500 | train loss: 889045.38\n",
      "Step:  1600 | train loss: 893039.94\n",
      "Step:  1700 | train loss: 983723.12\n",
      "Step:  1800 | train loss: 1019719.75\n",
      "Step:  1900 | train loss: 1157395.00\n",
      "Step:  2000 | train loss: 1071362.00\n",
      "Step:  2100 | train loss: 959640.31\n",
      "Step:  2200 | train loss: 530292.12\n",
      "updating new train loss:  346139.84\n",
      "updating new train loss:  338625.6\n",
      "updating new train loss:  329518.66\n",
      "updating new train loss:  301449.16\n",
      "updating new train loss:  295070.03\n",
      "updating new train loss:  270156.72\n",
      "updating new train loss:  245887.16\n",
      "updating new train loss:  237090.06\n",
      "updating new train loss:  228843.92\n",
      "updating new train loss:  222651.22\n",
      "updating new train loss:  216987.69\n",
      "updating new train loss:  214683.83\n",
      "updating new train loss:  212447.33\n",
      "updating new train loss:  208392.2\n",
      "updating new train loss:  204939.34\n",
      "updating new train loss:  191691.55\n",
      "updating new train loss:  178816.72\n",
      "updating new train loss:  176101.73\n",
      "updating new train loss:  166019.06\n",
      "Step:  2300 | train loss: 196320.92\n",
      "updating new train loss:  157472.11\n",
      "updating new train loss:  144127.72\n",
      "updating new train loss:  138669.58\n",
      "updating new train loss:  129547.91\n",
      "updating new train loss:  127374.98\n",
      "Step:  2400 | train loss: 303370.41\n",
      "Step:  2500 | train loss: 454954.09\n",
      "Step:  2600 | train loss: 517509.47\n",
      "Step:  2700 | train loss: 473577.75\n",
      "Step:  2800 | train loss: 715635.38\n",
      "Step:  2900 | train loss: 658671.19\n",
      "Step:  3000 | train loss: 564075.69\n",
      "Step:  3100 | train loss: 751668.62\n",
      "Step:  3200 | train loss: 820285.44\n",
      "Step:  3300 | train loss: 896464.75\n",
      "Step:  3400 | train loss: 1137400.75\n",
      "Step:  3500 | train loss: 1333421.25\n",
      "Step:  3600 | train loss: 1457342.12\n",
      "Step:  3700 | train loss: 1721885.38\n",
      "Step:  3800 | train loss: 1828636.38\n",
      "Step:  3900 | train loss: 1736933.50\n",
      "Step:  4000 | train loss: 1765725.38\n",
      "Step:  4100 | train loss: 1715281.00\n",
      "Step:  4200 | train loss: 1762987.00\n",
      "Step:  4300 | train loss: 2128919.50\n",
      "Step:  4400 | train loss: 2430624.50\n",
      "Step:  4500 | train loss: 2858262.00\n",
      "Step:  4600 | train loss: 3148020.00\n",
      "Step:  4700 | train loss: 3640536.75\n",
      "Step:  100 | train loss: 320151.56\n",
      "Step:  200 | train loss: 265595.34\n",
      "Step:  300 | train loss: 135238.11\n",
      "updating new train loss:  122954.1\n",
      "updating new train loss:  106967.99\n",
      "updating new train loss:  93009.63\n",
      "updating new train loss:  90449.65\n",
      "Step:  400 | train loss: 152204.16\n",
      "updating new train loss:  81723.805\n",
      "updating new train loss:  78342.72\n",
      "updating new train loss:  65510.84\n",
      "updating new train loss:  54012.184\n",
      "updating new train loss:  45739.49\n",
      "updating new train loss:  39292.582\n",
      "updating new train loss:  28905.068\n",
      "Step:  500 | train loss: 115350.48\n",
      "Step:  600 | train loss: 67121.00\n",
      "updating new train loss:  26516.402\n",
      "updating new train loss:  23327.773\n",
      "updating new train loss:  21373.773\n",
      "updating new train loss:  18299.316\n",
      "updating new train loss:  13788.731\n",
      "updating new train loss:  12053.627\n",
      "updating new train loss:  11873.682\n",
      "updating new train loss:  11455.802\n",
      "updating new train loss:  9467.647\n",
      "updating new train loss:  4987.413\n",
      "updating new train loss:  1841.5182\n",
      "updating new train loss:  583.30927\n",
      "updating new train loss:  257.8401\n",
      "updating new train loss:  232.47916\n",
      "Step:  700 | train loss: 7635.58\n",
      "Step:  800 | train loss: 2096.48\n",
      "Step:  900 | train loss: 24809.36\n",
      "Step:  1000 | train loss: 76396.84\n",
      "Step:  1100 | train loss: 68373.21\n",
      "Step:  1200 | train loss: 72710.79\n",
      "Step:  1300 | train loss: 127513.99\n",
      "Step:  1400 | train loss: 137611.55\n",
      "Step:  1500 | train loss: 150990.50\n",
      "Step:  1600 | train loss: 158616.02\n",
      "Step:  1700 | train loss: 202549.97\n",
      "Step:  1800 | train loss: 225523.45\n",
      "Step:  1900 | train loss: 301756.81\n",
      "Step:  2000 | train loss: 257892.39\n",
      "Step:  2100 | train loss: 208808.05\n",
      "Step:  2200 | train loss: 64193.18\n",
      "Step:  2300 | train loss: 85126.39\n",
      "Step:  2400 | train loss: 4385.82\n",
      "Step:  2500 | train loss: 31227.70\n",
      "Step:  2600 | train loss: 98807.44\n",
      "Step:  2700 | train loss: 40940.30\n",
      "Step:  2800 | train loss: 140029.95\n",
      "Step:  2900 | train loss: 111581.70\n",
      "Step:  3000 | train loss: 77097.76\n",
      "Step:  3100 | train loss: 157277.61\n",
      "Step:  3200 | train loss: 192591.88\n",
      "Step:  3300 | train loss: 230487.67\n",
      "Step:  3400 | train loss: 359496.16\n",
      "Step:  3500 | train loss: 470390.62\n",
      "Step:  3600 | train loss: 541378.00\n",
      "Step:  3700 | train loss: 703490.56\n",
      "Step:  3800 | train loss: 766550.31\n",
      "Step:  3900 | train loss: 703529.94\n",
      "Step:  4000 | train loss: 722936.06\n",
      "Step:  4100 | train loss: 686568.62\n",
      "Step:  4200 | train loss: 714783.12\n",
      "Step:  4300 | train loss: 951318.38\n",
      "Step:  4400 | train loss: 1151866.88\n",
      "Step:  4500 | train loss: 1446419.50\n",
      "Step:  4600 | train loss: 1648055.88\n",
      "Step:  4700 | train loss: 2003833.00\n",
      "Step:  100 | train loss: 16921.32\n",
      "Step:  200 | train loss: 5505.12\n",
      "Step:  300 | train loss: 17625.06\n",
      "Step:  400 | train loss: 14550.68\n",
      "Step:  500 | train loss: 19043.84\n",
      "Step:  600 | train loss: 45427.68\n",
      "Step:  700 | train loss: 138915.44\n",
      "Step:  800 | train loss: 163759.17\n",
      "Step:  900 | train loss: 77505.66\n",
      "Step:  1000 | train loss: 24435.84\n",
      "Step:  1100 | train loss: 25349.08\n",
      "Step:  1200 | train loss: 19597.47\n",
      "Step:  1300 | train loss: 1912.38\n",
      "Step:  1400 | train loss: 472.91\n",
      "updating new train loss:  222.14401\n",
      "updating new train loss:  83.42451\n",
      "updating new train loss:  11.340497\n",
      "updating new train loss:  6.1031647\n",
      "Step:  1500 | train loss: 129.31\n",
      "Step:  1600 | train loss: 2192.95\n",
      "Step:  1700 | train loss: 6460.48\n",
      "Step:  1800 | train loss: 14952.13\n",
      "Step:  1900 | train loss: 45157.51\n",
      "Step:  2000 | train loss: 26142.98\n",
      "Step:  2100 | train loss: 14796.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  2200 | train loss: 51045.42\n",
      "Step:  2300 | train loss: 286177.75\n",
      "Step:  2400 | train loss: 74091.11\n",
      "Step:  2500 | train loss: 15515.73\n",
      "Step:  2600 | train loss: 77839.27\n",
      "Step:  2700 | train loss: 6269.03\n",
      "Step:  2800 | train loss: 19958.94\n",
      "Step:  2900 | train loss: 4936.83\n",
      "Step:  3000 | train loss: 482.02\n",
      "Step:  3100 | train loss: 21358.61\n",
      "Step:  3200 | train loss: 38669.82\n",
      "Step:  3300 | train loss: 57178.60\n",
      "Step:  3400 | train loss: 129515.93\n",
      "Step:  3500 | train loss: 197336.22\n",
      "Step:  3600 | train loss: 241464.17\n",
      "Step:  3700 | train loss: 350612.81\n",
      "Step:  3800 | train loss: 391272.53\n",
      "Step:  3900 | train loss: 343861.12\n",
      "Step:  4000 | train loss: 359130.78\n",
      "Step:  4100 | train loss: 330776.47\n",
      "Step:  4200 | train loss: 349237.94\n",
      "Step:  4300 | train loss: 517870.38\n",
      "Step:  4400 | train loss: 665018.56\n",
      "Step:  4500 | train loss: 888921.69\n",
      "Step:  4600 | train loss: 1043571.31\n",
      "Step:  4700 | train loss: 1326342.88\n",
      "Step:  100 | train loss: 50568.48\n",
      "Step:  200 | train loss: 57341.98\n",
      "Step:  300 | train loss: 138364.97\n",
      "Step:  400 | train loss: 118174.23\n",
      "Step:  500 | train loss: 136519.34\n",
      "Step:  600 | train loss: 193027.72\n",
      "Step:  700 | train loss: 353747.53\n",
      "Step:  800 | train loss: 384686.72\n",
      "Step:  900 | train loss: 237695.22\n",
      "Step:  1000 | train loss: 127575.05\n",
      "Step:  1100 | train loss: 126838.70\n",
      "Step:  1200 | train loss: 110473.80\n",
      "Step:  1300 | train loss: 53334.39\n",
      "Step:  1400 | train loss: 41473.13\n",
      "Step:  1500 | train loss: 30005.28\n",
      "Step:  1600 | train loss: 25100.51\n",
      "Step:  1700 | train loss: 7298.28\n",
      "Step:  1800 | train loss: 6031.56\n",
      "Step:  1900 | train loss: 13726.88\n",
      "Step:  2000 | train loss: 3292.04\n",
      "Step:  2100 | train loss: 4924.93\n",
      "Step:  2200 | train loss: 111219.24\n",
      "Step:  2300 | train loss: 428005.28\n",
      "Step:  2400 | train loss: 158718.14\n",
      "Step:  2500 | train loss: 61453.22\n",
      "Step:  2600 | train loss: 118074.58\n",
      "Step:  2700 | train loss: 37569.58\n",
      "Step:  2800 | train loss: 13586.88\n",
      "Step:  2900 | train loss: 1698.29\n",
      "Step:  3000 | train loss: 6450.65\n",
      "Step:  3100 | train loss: 2612.11\n",
      "Step:  3200 | train loss: 12269.53\n",
      "Step:  3300 | train loss: 23978.54\n",
      "Step:  3400 | train loss: 79296.56\n",
      "Step:  3500 | train loss: 137044.42\n",
      "Step:  3600 | train loss: 176992.81\n",
      "Step:  3700 | train loss: 274498.41\n",
      "Step:  3800 | train loss: 311657.03\n",
      "Step:  3900 | train loss: 270266.50\n",
      "Step:  4000 | train loss: 285627.72\n",
      "Step:  4100 | train loss: 260179.30\n",
      "Step:  4200 | train loss: 276916.25\n",
      "Step:  4300 | train loss: 429028.66\n",
      "Step:  4400 | train loss: 563356.56\n",
      "Step:  4500 | train loss: 770032.06\n",
      "Step:  4600 | train loss: 913333.38\n",
      "Step:  4700 | train loss: 1178141.62\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 4\n",
    "BATCH_SIZE = 1\n",
    "# TIME_STEP = 10\n",
    "INPUT_SIZE = 5\n",
    "LR = 0.005\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 30,\n",
    "            num_layers = 2,\n",
    "            batch_first = True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.rnn2 = nn.RNN(\n",
    "            input_size = 5,\n",
    "            hidden_size = 30,\n",
    "            num_layers = 2,\n",
    "            batch_first = True\n",
    "        )\n",
    "            \n",
    "        self.out = nn.Linear(30, 5) # is the first number here = hidden_size? and second = 1 for regression?\n",
    "        \n",
    "    def forward(self, x, h_state):\n",
    "        \n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        \n",
    "        r_out, h_state = self.rnn2(x, h_state)\n",
    "        \n",
    "        r_out = r_out.view(-1,30)\n",
    "        \n",
    "        outs = self.out(r_out)\n",
    "        \n",
    "        return outs, h_state\n",
    "    \n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = LR)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "h_state = None\n",
    "min_train_loss = 999999999\n",
    "loss_list = []\n",
    "prediction_save = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for i in range(1, spy_df.size(1)-1):\n",
    "    #     print(i)\n",
    "        x = spy_df[:,((i-1)*BATCH_SIZE):(i * BATCH_SIZE),:]\n",
    "        y = spy_df[:,(i * BATCH_SIZE+1),:]\n",
    "#         y = spy_df[:,(i * BATCH_SIZE+1),:]\n",
    "\n",
    "        prediction, h_state = rnn(x, h_state)\n",
    "        h_state = h_state.data\n",
    "        prediction_save.append(prediction)\n",
    "\n",
    "        loss = loss_func(prediction, y)\n",
    "        optimizer.zero_grad()                   # clear gradients for this training step\n",
    "        loss.backward()                         # backpropagation, compute gradients\n",
    "        optimizer.step() \n",
    "        \n",
    "        if loss.data.numpy() <= min_train_loss:\n",
    "            min_train_loss = loss.data.numpy()\n",
    "            print(\"updating new train loss: \", loss.data.numpy())\n",
    "        loss_list.append(loss.data.numpy())\n",
    "        if i %100 == 0:\n",
    "            print('Step: ', i, '| train loss: %.2f' % loss.data.numpy())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1031647\n"
     ]
    }
   ],
   "source": [
    "print(min_train_loss) \n",
    "# 22100 steps and 44200 steps gives similar loss at 15 with LR = 0.01\n",
    "# 44200 steps with LR = 0.005 gives loss at 1.82\n",
    "# 88400 steps with LR = 0.0025 gives loss at 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  tensor([1672.7499, 1681.1732, 1663.8157, 1673.1177,  355.4963],\n",
      "       grad_fn=<SelectBackward>)\n",
      "y label :  tensor([2902.5400, 2909.6399, 2869.2900, 2885.5701,  332.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction: \",prediction[-1])\n",
    "print(\"y label : \",y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47190"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1171087f0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(1248379.6, dtype=float32),\n",
       " array(1246592.8, dtype=float32),\n",
       " array(1237217.4, dtype=float32),\n",
       " array(1239448.9, dtype=float32),\n",
       " array(1233417.8, dtype=float32),\n",
       " array(1259782., dtype=float32),\n",
       " array(1255280.4, dtype=float32),\n",
       " array(1264927.5, dtype=float32),\n",
       " array(1217749.1, dtype=float32),\n",
       " array(1189055.1, dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1445402.1250, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(prediction, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1445402.1, dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(prediction, y).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
