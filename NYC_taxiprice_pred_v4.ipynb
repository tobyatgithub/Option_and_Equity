{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n9-29-18, version 4:\\nIn last version, we figured out how to progressively split data, \\nand how to make tf progressively read and train on large dataset.\\n# last rsme with 5 raw features w/ LineraR = 9.8\\n\\nIn this version, we want to add feature engineering onto it,\\nand hopefully improve the rsme largely.\\n\\nOk..met one very strange error as the fare_amount input at step 6701 with batch size = 128*3.\\nNot sure what was wrong about.. and what made it worse... it seems hard to print out the error line\\nin the fly of tf estimator to see what went wrong.\\n\\n###################\\n9-14-18:\\nIn v1 file, we built the model with few extra feature and used the simplest model (linear regression)\\n\\nIn last v2 file, we added more features (both bucket location, and several time features),\\nwe also applied a DNN instead of linear regression to train the model.\\n\\nIn this v3 file, we want to improve the data input---we are not satisfied by just reading\\nthe first million data. As a result, we want to read that large dataset progressively.\\nReference Source = \\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/03_tensorflow/c_dataset.ipynb\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "9-29-18, version 4:\n",
    "In last version, we figured out how to progressively split data, \n",
    "and how to make tf progressively read and train on large dataset.\n",
    "# last rsme with 5 raw features w/ LineraR = 9.8\n",
    "\n",
    "In this version, we want to add feature engineering onto it,\n",
    "and hopefully improve the rsme largely.\n",
    "\n",
    "Ok..met one very strange error as the fare_amount input at step 6701 with batch size = 128*3.\n",
    "Not sure what was wrong about.. and what made it worse... it seems hard to print out the error line\n",
    "in the fly of tf estimator to see what went wrong.\n",
    "\n",
    "###################\n",
    "9-14-18:\n",
    "In v1 file, we built the model with few extra feature and used the simplest model (linear regression)\n",
    "\n",
    "In last v2 file, we added more features (both bucket location, and several time features),\n",
    "we also applied a DNN instead of linear regression to train the model.\n",
    "\n",
    "In this v3 file, we want to improve the data input---we are not satisfied by just reading\n",
    "the first million data. As a result, we want to read that large dataset progressively.\n",
    "Reference Source = \n",
    "https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/03_tensorflow/c_dataset.ipynb\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirctory = \"/Users/toby/Downloads/Data/Kaggle_Taxi/\"\n",
    "# tensorboard --logdir=/Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1\n",
    "# localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toby/anaconda3/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/toby/Documents/PycharmProjects/Kaggle/NYC_taxiFare_Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import time\n",
    "from math import sqrt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data =  pd.read_csv(dirctory + 'chunk_eval_v2.csv', nrows = 10000, header = None)\n",
    "# data.to_csv(dirctory + 'chunk_eval_v2_short.csv', index = False, mode = \"w\", header = False)\n",
    "# # data =  pd.read_csv('nyc_taxi_train.csv', nrows = 1000)\n",
    "\n",
    "# print(data.info())\n",
    "# data.head()\n",
    "# train_df, eval_df = train_test_split(data, test_size = 0.4)\n",
    "# print(train_df.dtypes)\n",
    "# train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the supplying functions \n",
    "def late_night (row):\n",
    "    if (row['hour'] <= 6) or (row['hour'] >= 20):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def night (row):\n",
    "    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    df = df.dropna(how = 'any', axis = 'rows')\n",
    "    df = df[df[\"fare_amount\"] > 2.5]\n",
    "    df = df[(df[\"passenger_count\"] > 0) & (df[\"passenger_count\"] <= 4)]\n",
    "    df = df[(df[\"dropoff_longitude\"] > -76) & (df[\"dropoff_longitude\"] < -72)]\n",
    "    df = df[(df[\"pickup_longitude\"] > -76) & (df[\"pickup_longitude\"] < -72)]\n",
    "    df = df[(df[\"pickup_latitude\"] > 38) & (df[\"pickup_latitude\"] < 42)]\n",
    "    df = df[(df[\"dropoff_latitude\"] > 38) & (df[\"dropoff_latitude\"] < 42)]  \n",
    "    \n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "    \n",
    "    df['distance'] = ((df.abs_diff_longitude)**2 + \\\n",
    "                      (df.abs_diff_latitude)**2).apply(lambda x: 69.172*sqrt(x))\n",
    "\n",
    "    df = df[((df[\"distance\"] <= 25) & (df[\"fare_amount\"] <= 50))\\\n",
    "            | ((df[\"distance\"] > 25) & (df[\"fare_amount\"] >= 50))]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_more_features(df):\n",
    "    \"\"\"here..\"\"\"\n",
    "    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n",
    "    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n",
    "    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n",
    "    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n",
    "    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n",
    "    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n",
    "    df['night'] = df.apply (lambda x: night(x), axis=1)\n",
    "    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)    \n",
    "    df = df.drop(\"pickup_datetime\",1)\n",
    "    return df\n",
    "\n",
    "# Create pandas input function\n",
    "def make_input_fn(df, num_epochs):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = add_more_features(df),\n",
    "    y = df['fare_amount'],\n",
    "    batch_size = 128*3, # no sure why..\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )\n",
    "\n",
    "def create_feature_cols():\n",
    "    return [\n",
    "    tf.feature_column.numeric_column('passenger_count'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_longitude'),\\\n",
    "                                        boundaries = np.arange(-76.0, -72, 0.25).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('dropoff_longitude'),\\\n",
    "                                        boundaries = np.arange(-76.0, -72, 0.25).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_latitude'),\\\n",
    "                                        boundaries = np.arange(38.0, 42, 0.25).tolist()), \n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('dropoff_latitude'),\\\n",
    "                                        boundaries = np.arange(38.0, 42, 0.25).tolist()),         \n",
    "    tf.feature_column.numeric_column('abs_diff_longitude'),\n",
    "    tf.feature_column.numeric_column('abs_diff_latitude'),\n",
    "    tf.feature_column.numeric_column('distance'),\n",
    "    tf.feature_column.numeric_column('year'),\n",
    "    tf.feature_column.numeric_column('month'),\n",
    "    tf.feature_column.numeric_column('day'),\n",
    "    tf.feature_column.numeric_column('hour'),\n",
    "    tf.feature_column.numeric_column('weekday'),\n",
    "    tf.feature_column.numeric_column('night'),\n",
    "    tf.feature_column.numeric_column('late_night')        \n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tf.decode_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV_COLUMNS = ['key','fare_amount', 'pickup_datetime', 'pickup_longitude',\n",
    "#                'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "#                'passenger_count']\n",
    "# DEFAULTS = [['nokey'],[0.0], [\"None\"], [-74.0], [40.0], [-74.0], [40.7], [1.0], ]\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    'key', 'fare_amount', 'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'passenger_count',\n",
    "    'abs_diff_longitude', 'abs_diff_latitude', 'distance', 'year', 'month',\n",
    "    'day', 'hour', 'weekday', 'night', 'late_night']\n",
    "\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [['None'],[0.0], [-74.0], [40.0], [-74.0], [40.0], [1.0], \n",
    "            [0.0],[0.0],[0.0],[0],[0],[0],[0],[0],[0],[0]]\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512 * 2):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS) \n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        dataset = tf.data.TextLineDataset(filename).skip(1).map(decode_csv)\n",
    "\n",
    "        # Note:\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "\n",
    "def get_train():\n",
    "    return read_dataset(dirctory + 'chunk_train_v2.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "#     return read_dataset(r'nyc_taxi_df_train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "    return read_dataset(dirctory + 'chunk_eval_v2_short.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
    "#     return read_dataset(r'nyc_taxi_df_eval.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "def get_test():\n",
    "    return read_dataset(dirctory + 'featured_test.csv', mode = tf.estimator.ModeKeys.PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tf.estimator.LinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3 old:\n",
    "# INPUT_COLUMNS = [\n",
    "#     tf.feature_column.numeric_column('pickup_longitude'),\n",
    "#     tf.feature_column.numeric_column('pickup_latitude'),\n",
    "#     tf.feature_column.numeric_column('dropoff_latitude'),\n",
    "#     tf.feature_column.numeric_column('dropoff_longitude'),\n",
    "#     tf.feature_column.numeric_column('passenger_count'),\n",
    "# ]\n",
    "\n",
    "# v4 new:\n",
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('passenger_count'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_longitude'),\\\n",
    "                                        boundaries = np.arange(-76.0, -72, 0.25).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('dropoff_longitude'),\\\n",
    "                                        boundaries = np.arange(-76.0, -72, 0.25).tolist()),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('pickup_latitude'),\\\n",
    "                                        boundaries = np.arange(38.0, 42, 0.25).tolist()), \n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('dropoff_latitude'),\\\n",
    "                                        boundaries = np.arange(38.0, 42, 0.25).tolist()),         \n",
    "    tf.feature_column.numeric_column('abs_diff_longitude'),\n",
    "    tf.feature_column.numeric_column('abs_diff_latitude'),\n",
    "    tf.feature_column.numeric_column('distance'),\n",
    "    tf.feature_column.numeric_column('year'),\n",
    "    tf.feature_column.numeric_column('month'),\n",
    "    tf.feature_column.numeric_column('day'),\n",
    "    tf.feature_column.numeric_column('hour'),\n",
    "    tf.feature_column.numeric_column('weekday'),\n",
    "    tf.feature_column.numeric_column('night'),\n",
    "    tf.feature_column.numeric_column('late_night')        \n",
    "  ]\n",
    "\n",
    "def add_more_features(feats):\n",
    "    # Nothing to add (yet!)\n",
    "    return feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this for first time training...\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "OUTDIR = dirctory + 'taxi_local_v1'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "# model = tf.estimator.LinearRegressor(feature_columns = feature_cols, model_dir = OUTDIR)\n",
    "model = tf.estimator.DNNRegressor(feature_columns = feature_cols, \n",
    "                                  model_dir = OUTDIR,\n",
    "                                  hidden_units=[1024,512,512,256],\n",
    "                                  optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                      learning_rate=0.01,\n",
    "                                      l2_regularization_strength=0.02)\n",
    "                                 )\n",
    "model.train(input_fn = get_train(), steps = 8000) # use steps limit to tune the parameters\n",
    "# model.train(input_fn = get_train(), max_steps=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? tf.estimator.LinearRegressor.train\n",
    "# ? pd.DataFrame.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11e4cd080>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1/model.ckpt.\n",
      "INFO:tensorflow:loss = 5979916.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 13.1097\n",
      "INFO:tensorflow:loss = 52186.85, step = 101 (7.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7042\n",
      "INFO:tensorflow:loss = 53047.1, step = 201 (7.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8335\n",
      "INFO:tensorflow:loss = 60401.555, step = 301 (7.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3397\n",
      "INFO:tensorflow:loss = 56388.984, step = 401 (6.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5364\n",
      "INFO:tensorflow:loss = 59480.13, step = 501 (6.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5121\n",
      "INFO:tensorflow:loss = 56579.945, step = 601 (6.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2314\n",
      "INFO:tensorflow:loss = 70073.84, step = 701 (7.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4788\n",
      "INFO:tensorflow:loss = 50326.88, step = 801 (6.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3852\n",
      "INFO:tensorflow:loss = 50786.75, step = 901 (6.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2662\n",
      "INFO:tensorflow:loss = 45056.016, step = 1001 (7.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6664\n",
      "INFO:tensorflow:loss = 40037.75, step = 1101 (7.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.645\n",
      "INFO:tensorflow:loss = 91553.83, step = 1201 (6.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2344\n",
      "INFO:tensorflow:loss = 37457.78, step = 1301 (7.555 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.601\n",
      "INFO:tensorflow:loss = 31635.191, step = 1401 (7.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8806\n",
      "INFO:tensorflow:loss = 28025.293, step = 1501 (6.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9444\n",
      "INFO:tensorflow:loss = 24119.016, step = 1601 (6.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9498\n",
      "INFO:tensorflow:loss = 28595.605, step = 1701 (6.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.788\n",
      "INFO:tensorflow:loss = 20513.035, step = 1801 (6.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9643\n",
      "INFO:tensorflow:loss = 21135.355, step = 1901 (6.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9132\n",
      "INFO:tensorflow:loss = 23495.244, step = 2001 (6.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9748\n",
      "INFO:tensorflow:loss = 17479.254, step = 2101 (6.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8271\n",
      "INFO:tensorflow:loss = 21792.914, step = 2201 (6.744 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9876\n",
      "INFO:tensorflow:loss = 16191.788, step = 2301 (6.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9862\n",
      "INFO:tensorflow:loss = 14612.484, step = 2401 (6.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9362\n",
      "INFO:tensorflow:loss = 21408.645, step = 2501 (6.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9499\n",
      "INFO:tensorflow:loss = 14997.385, step = 2601 (6.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7559\n",
      "INFO:tensorflow:loss = 15204.907, step = 2701 (6.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9387\n",
      "INFO:tensorflow:loss = 15139.429, step = 2801 (6.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9404\n",
      "INFO:tensorflow:loss = 14119.244, step = 2901 (6.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9341\n",
      "INFO:tensorflow:loss = 10767.729, step = 3001 (6.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7451\n",
      "INFO:tensorflow:loss = 12121.131, step = 3101 (6.783 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9163\n",
      "INFO:tensorflow:loss = 11866.289, step = 3201 (6.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.905\n",
      "INFO:tensorflow:loss = 10794.275, step = 3301 (6.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9306\n",
      "INFO:tensorflow:loss = 11477.801, step = 3401 (6.698 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9927\n",
      "INFO:tensorflow:loss = 11987.986, step = 3501 (6.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7297\n",
      "INFO:tensorflow:loss = 12661.118, step = 3601 (6.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9617\n",
      "INFO:tensorflow:loss = 15662.639, step = 3701 (6.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9433\n",
      "INFO:tensorflow:loss = 9557.022, step = 3801 (6.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9677\n",
      "INFO:tensorflow:loss = 10256.837, step = 3901 (6.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8187\n",
      "INFO:tensorflow:loss = 18393.488, step = 4001 (6.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9759\n",
      "INFO:tensorflow:loss = 16211.787, step = 4101 (6.678 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9186\n",
      "INFO:tensorflow:loss = 9834.74, step = 4201 (6.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8851\n",
      "INFO:tensorflow:loss = 13642.842, step = 4301 (6.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8787\n",
      "INFO:tensorflow:loss = 10776.803, step = 4401 (7.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0508\n",
      "INFO:tensorflow:loss = 16383.057, step = 4501 (7.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6335\n",
      "INFO:tensorflow:loss = 16539.098, step = 4601 (7.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6649\n",
      "INFO:tensorflow:loss = 12950.49, step = 4701 (7.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4924\n",
      "INFO:tensorflow:loss = 13355.184, step = 4801 (7.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6688\n",
      "INFO:tensorflow:loss = 10905.977, step = 4901 (8.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1146\n",
      "INFO:tensorflow:loss = 15344.217, step = 5001 (7.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.789\n",
      "INFO:tensorflow:loss = 12249.357, step = 5101 (7.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3731\n",
      "INFO:tensorflow:loss = 9921.826, step = 5201 (6.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5076\n",
      "INFO:tensorflow:loss = 14005.293, step = 5301 (6.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3566\n",
      "INFO:tensorflow:loss = 9220.601, step = 5401 (6.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9765\n",
      "INFO:tensorflow:loss = 13498.781, step = 5501 (7.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8048\n",
      "INFO:tensorflow:loss = 11266.482, step = 5601 (6.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7397\n",
      "INFO:tensorflow:loss = 16763.045, step = 5701 (6.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7985\n",
      "INFO:tensorflow:loss = 17542.01, step = 5801 (6.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8575\n",
      "INFO:tensorflow:loss = 11658.669, step = 5901 (6.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8862\n",
      "INFO:tensorflow:loss = 11599.94, step = 6001 (6.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8022\n",
      "INFO:tensorflow:loss = 13785.11, step = 6101 (6.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.913\n",
      "INFO:tensorflow:loss = 17028.125, step = 6201 (6.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8561\n",
      "INFO:tensorflow:loss = 9180.865, step = 6301 (6.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8605\n",
      "INFO:tensorflow:loss = 8240.631, step = 6401 (6.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.5674\n",
      "INFO:tensorflow:loss = 14634.165, step = 6501 (6.865 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.6885\n",
      "INFO:tensorflow:loss = 11647.734, step = 6601 (6.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8362\n",
      "INFO:tensorflow:loss = 11589.949, step = 6701 (6.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.872\n",
      "INFO:tensorflow:loss = 11313.183, step = 6801 (6.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9067\n",
      "INFO:tensorflow:loss = 10912.095, step = 6901 (6.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8109\n",
      "INFO:tensorflow:loss = 9231.249, step = 7001 (6.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8955\n",
      "INFO:tensorflow:loss = 16379.148, step = 7101 (6.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8656\n",
      "INFO:tensorflow:loss = 12516.338, step = 7201 (6.726 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 14.8859\n",
      "INFO:tensorflow:loss = 13445.65, step = 7301 (6.718 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8978\n",
      "INFO:tensorflow:loss = 10899.579, step = 7401 (6.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8039\n",
      "INFO:tensorflow:loss = 12071.794, step = 7501 (6.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2834\n",
      "INFO:tensorflow:loss = 14863.404, step = 7601 (7.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.795\n",
      "INFO:tensorflow:loss = 10238.288, step = 7701 (6.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7703\n",
      "INFO:tensorflow:loss = 10674.867, step = 7801 (6.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.7256\n",
      "INFO:tensorflow:loss = 14039.779, step = 7901 (6.790 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into /Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9682.946.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNRegressor at 0x11e4404a8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this for continuing training\n",
    "\n",
    "# restore training 9-30-2018\n",
    "# since the training didn't finish last time\n",
    "# OUTDIR = dirctory + 'taxi_local_v1'\n",
    "# model = tf.estimator.DNNRegressor(feature_columns = feature_cols, \n",
    "#                                   model_dir = OUTDIR,\n",
    "#                                   hidden_units=[512,512,256],\n",
    "#                                   optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "#                                       learning_rate=0.01,\n",
    "#                                       l2_regularization_strength=0.02)\n",
    "#                                  )\n",
    "# model.train(input_fn = get_train(), steps = 8000) # use steps limit to tune the parameters\n",
    "# model.train(input_fn = get_train(), max_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from /Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1/model.ckpt-8000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export/temp-b'1538729042'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'export/1538729042'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "feature_space = tf.feature_column.make_parse_example_spec(feature_cols)\n",
    "\n",
    "serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_space)\n",
    "model.export_savedmodel('export', serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-05-08:44:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1/model.ckpt-8000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-05-08:44:03\n",
      "INFO:tensorflow:Saving dict for global step 8000: average_loss = 12.850069, global_step = 8000, label/mean = 10.67352, loss = 12848.784, prediction/mean = 11.0478115\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8000: /Users/toby/Downloads/Data/Kaggle_Taxi/taxi_local_v1/model.ckpt-8000\n",
      "RMSE on eval dataset = 3.5846993923187256\n"
     ]
    }
   ],
   "source": [
    "def print_rmse(model, name, input_fn):\n",
    "    metrics = model.evaluate(input_fn = input_fn, steps = None)\n",
    "    print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\n",
    "print_rmse(model, 'eval', get_valid()) #well...this evaluation is realllly taking long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes:\n",
    "# 1. [1024,512,512,256], 0.01, l1 w/ 0.02, train step = 5000 batch = 512 * 2, 4.2\n",
    "# 2. [1024,512,256], 0.01, l1 w/ 0.02, train step = 8000 batch = 512 * 2, 3.55\n",
    "# 3. [512,512,256], 0.01, l2 w/ 0.02, train step = 8000 batch = 512 * 2, 3.58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_pred_input_fn(df, num_epochs):\n",
    "#     return tf.estimator.inputs.pandas_input_fn(\n",
    "#     x = df,\n",
    "#     y = None,\n",
    "#     batch_size = 128,\n",
    "#     num_epochs = num_epochs,\n",
    "#     shuffle = False,\n",
    "#     queue_capacity = 1000,\n",
    "#     num_threads = 1)\n",
    "\n",
    "# test_df = pd.read_csv(dirctory + \"featured_test.csv\")\n",
    "# test_df.head()\n",
    "\n",
    "# # model stopped around step 720301 is saved to b'export/1538361964'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model.predict(input_fn = make_pred_input_fn(test_df,1))\n",
    "# result_df = pd.DataFrame(result)\n",
    "# result_df['predictions'] = result_df['predictions'].apply(lambda x: float(x))\n",
    "# result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model.predict(input_fn = make_pred_input_fn(test_df,1))\n",
    "# result_df = pd.DataFrame(result)\n",
    "# result_df['predictions'] = result_df['predictions'].apply(lambda x: float(x))\n",
    "# result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = result_df.merge(test_df, left_index=True, right_index=True)[[\"predictions\",\"key\"]]\n",
    "# sub.set_index(\"key\", inplace = True)\n",
    "# sub.columns = [\"fare_amount\"]\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.to_csv(dirctory + \"taxi_submission_full.csv\", mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model.predict(input_fn = get_test())\n",
    "# result_df = pd.DataFrame(result)\n",
    "\n",
    "# result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTDIR = 'taxi_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.system(\"Toby, 'job run finished!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
